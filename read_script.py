import faiss
import numpy as np
from PyPDF2 import PdfReader
from sentence_transformers import SentenceTransformer
import openai

# Set your OpenAI API key
openai.api_key = "open-ai-key"

# Load the SentenceTransformer model
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

def extract_text_from_pdf(pdf_path):
    """Extracts text from a PDF file."""
    reader = PdfReader(pdf_path)
    text = ""
    for page in reader.pages:
        text += page.extract_text()
    return text

def split_text_into_chunks(text, chunk_size=500):
    """Splits text into chunks of specified size."""
    words = text.split()
    chunks = [" ".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]
    return chunks

def generate_embeddings(chunks):
    """Generates embeddings for each text chunk."""
    embeddings = model.encode(chunks, convert_to_numpy=True)
    return embeddings

def store_embeddings_in_faiss(embeddings):
    """Stores embeddings in a FAISS index."""
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)
    print(f"Number of embeddings stored in FAISS: {index.ntotal}")
    return index

def search_faiss(index, query, k=3):
    """Searches the FAISS index for the top-k most similar embeddings."""
    query_embedding = model.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_embedding, k)
    return indices[0], distances[0]

def generate_detailed_response_with_gpt(query, index, chunks, k=3):
    """
    Generates a precise response using OpenAI's GPT model.

    Parameters:
    - query: The user's query.
    - index: FAISS index containing embeddings.
    - chunks: The text chunks.
    - k: Number of relevant chunks to retrieve.

    Returns:
    - A detailed response generated by GPT.
    """
    # Step 1: Retrieve the most relevant chunks
    indices, distances = search_faiss(index, query, k=k)
    relevant_chunks = [chunks[idx] for idx in indices]

    # Step 2: Create a detailed prompt for GPT
    context = "\n".join(relevant_chunks)
    prompt = (f"The following text is relevant to the user's query:\n\n{context}\n\n"
              f"User Query: {query}\n\n"
              f"Provide a detailed, precise, and comprehensive response based on the provided text.")
    
    # Step 3: Use GPT-4 to generate a refined response
    response = openai.chat.completions.create(
        model="gpt-4o",  # Use "gpt-3.5-turbo" if you don't have GPT-4 access
        messages=[
            {"role": "system", "content": "You are an intelligent assistant that provides detailed and accurate answers."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=500
    )
    
    return response.choices[0].message['content']

# Example usage
pdf_path = "your file"  # Replace with your PDF file
text = extract_text_from_pdf(pdf_path)
chunks = split_text_into_chunks(text, chunk_size=300)
embeddings = generate_embeddings(chunks)
index = store_embeddings_in_faiss(embeddings)

# User Query
query = "Explain our business"
response = generate_detailed_response_with_gpt(query, index, chunks)
print("\nGenerated Response:\n", response)
